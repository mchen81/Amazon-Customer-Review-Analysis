{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most hot products of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the metadata at first\n",
    "metaDF = spark.read.json(\"hdfs://orion11:21001/parsed-meta/\")\n",
    "\n",
    "metaDF.createOrReplaceTempView(\"viewMetaData\")\n",
    "\n",
    "metaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+------------------+\n",
      "|               title|      asin|count|     average_score|\n",
      "+--------------------+----------+-----+------------------+\n",
      "|Powerstep Pinnacl...|B000V0IBDM| 4279| 4.515073615330684|\n",
      "|Powerstep Pinnacl...|B000KPIHQ4| 4270| 4.517330210772834|\n",
      "|90 Degree By Refl...|B00I0VHS10| 3608| 4.428215077605321|\n",
      "|MJ Metals Jewelry...|B00RLSCLJM| 3577| 4.826670394185071|\n",
      "|Hanes Mens EcoSma...|B000YFSR5G| 2420|  3.81198347107438|\n",
      "|i play. Baby Boys...|B000PHANNM| 2401| 4.658892128279883|\n",
      "| i play. Baby Sol...|B00201ER88| 2081| 4.423354156655454|\n",
      "|Best RFID Blockin...|B00GXE331K| 1873| 4.225840896956754|\n",
      "|i play. Girls' Ba...|B000P0X15G| 1823| 4.415249588590236|\n",
      "|Marino Avenue Men...|B00XT15P8E| 1790| 4.732960893854749|\n",
      "|Totes Kids Bubble...|B0017U1KBK| 1710| 4.194152046783626|\n",
      "|Ingrid &amp; Isab...|B005N7YWX6| 1640|4.3012195121951216|\n",
      "|Hanes Mens EcoSma...|B000YFSR4W| 1566|3.8371647509578546|\n",
      "|BodyJ4You Lot of ...|B004HX6P1E| 1544|4.3270725388601035|\n",
      "|Vans Adult Classi...|B000JOOR7O| 1535| 4.409120521172638|\n",
      "|Scarleton Large S...|B009RUKQ2G| 1500| 4.198666666666667|\n",
      "|LaSuiveur Womens ...|B00ZW3SCF0| 1490|3.9885906040268457|\n",
      "|Carhartt Men's Lo...|B000GHMRLW| 1370| 4.313868613138686|\n",
      "|Carhartt Men's Lo...|B000GHRZN2| 1370| 4.313868613138686|\n",
      "|O'Neill Men's Son...|B00NIVAEG8| 1334| 4.272863568215892|\n",
      "+--------------------+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: AMAZON_FASHION\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/AMAZON_FASHION.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----+------------------+\n",
      "|               title|      asin|count|     average_score|\n",
      "+--------------------+----------+-----+------------------+\n",
      "|Waterpik Ultra Wa...|B000FOI48G| 7711| 4.446894047464661|\n",
      "|Waterpik Ultra Wa...|B000GLRREU| 7476| 4.448234349919743|\n",
      "|                null|1620213982| 4532|4.7950132391879965|\n",
      "|Astra Platinum Do...|B001QY8QXM| 4270|4.5688524590163935|\n",
      "|Bali Secrets Natu...|B01DKQAXC0| 4090| 4.217848410757946|\n",
      "|Aquaphor Healing ...|B006IB5T4W| 2260| 4.664159292035398|\n",
      "|Panasonic Bikini ...|B00005JS5C| 2154| 4.088672237697307|\n",
      "|Panasonic Bikini ...|B00005JS5C| 2154| 4.088672237697307|\n",
      "|Helen of Troy 157...|B000WYJTZG| 2091|3.9775227164036346|\n",
      "|Pre de Provence A...|B00W259T7G| 1987| 4.498238550578762|\n",
      "|Bath &amp; Body W...|B0012Y0ZG2| 1910| 4.843455497382199|\n",
      "|Italia Deluxe Ult...|B00VF344X0| 1875|4.3802666666666665|\n",
      "|Style Edit Root C...|B00BMVV3MK| 1702| 4.503525264394829|\n",
      "|Eyelash Growth Se...|B0067F28ZW| 1644|3.8899026763990268|\n",
      "|Braun Clean &amp;...|B000050FDY| 1417| 4.658433309809457|\n",
      "|Braun Clean &amp;...|B000050FDY| 1417| 4.658433309809457|\n",
      "|Philips Norelco G...|B000FED5DU| 1128| 4.284574468085107|\n",
      "|FOONEE Rhinestone...|B008U1Q4DI| 1128|  4.37145390070922|\n",
      "|Philips Norelco 7...|B001AJ6YS2| 1037| 4.215043394406943|\n",
      "|Poppy Austin Pure...|B00D3M0CRS| 1034| 4.608317214700193|\n",
      "+--------------------+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2: All_Beauty\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/All_Beauty.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Cell_Phones_and_Accessories\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Cell_Phones_and_Accessories.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Electronics\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Electronics.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Home_and_Kitchen\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Home_and_Kitchen.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: Sports_and_Outdoors\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Sports_and_Outdoors.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7: Video_Games\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Video_Games.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the average review score of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(overall)|\n",
      "+------------------+\n",
      "|3.9069401880412298|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: AMAZON_FASHION\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/AMAZON_FASHION.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     avg(overall)|\n",
      "+-----------------+\n",
      "|4.112092528511223|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2: All_Beauty\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/All_Beauty.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     avg(overall)|\n",
      "+-----------------+\n",
      "|3.933553308546787|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3: Cell_Phones_and_Accessories\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Cell_Phones_and_Accessories.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     avg(overall)|\n",
      "+-----------------+\n",
      "|4.073685099988554|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#4: Electronics\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Electronics.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     avg(overall)|\n",
      "+-----------------+\n",
      "|4.194893574445901|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5: Home_and_Kitchen\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Home_and_Kitchen.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count , AVG(overall) as average_score FROM AMAZON_FASHION WHERE verified = True GROUP BY asin ORDER BY count DESC LIMIT 20\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, count, average_score FROM sqlDF1 LEFT JOIN viewMetaData ON sqlDF1.asin = viewMetadata.product_id ORDER BY count DESC\")\n",
    "\n",
    "showDF1.show()\n",
    "\n",
    "#sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|     avg(overall)|\n",
      "+-----------------+\n",
      "|4.243398403354114|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#6: Sports_and_Outdoors\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Sports_and_Outdoors.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|      avg(overall)|\n",
      "+------------------+\n",
      "|4.0220948494727224|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7: Video_Games\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Video_Games.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT AVG(overall) FROM AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
