{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|      asin|count|     average_score|\n",
      "+----------+-----+------------------+\n",
      "|B01EC05AZI|  515|2.7398058252427187|\n",
      "|B00UJ6GUYA| 1004|  2.99800796812749|\n",
      "|B001T4MSU6|  527| 3.301707779886148|\n",
      "|B0183JQHCO| 1226|3.3556280587275693|\n",
      "|B00UDF11O6|  738|3.3672086720867207|\n",
      "|B009PMILQE|  855| 3.433918128654971|\n",
      "|B0045D7VZ0|  560|              3.45|\n",
      "|B000EE1NNA|  520|3.4692307692307693|\n",
      "|B00E1L9QOU|  656|3.4923780487804876|\n",
      "|B00XV52VMG|  538| 3.676579925650558|\n",
      "|B018KRRC2I|  854| 3.728337236533958|\n",
      "|B019Q577XC|  735| 3.763265306122449|\n",
      "|B005MF5NE6|  546|3.8058608058608057|\n",
      "|B000YFSR5G| 2420|  3.81198347107438|\n",
      "|B00YP2TNZ2|  573| 3.825479930191972|\n",
      "|B000YFSR4W| 1566|3.8371647509578546|\n",
      "|B0014F8TIU|  878| 3.856492027334852|\n",
      "|B007F55UGW|  506|3.9209486166007905|\n",
      "|B00DHCZVMK|  594| 3.951178451178451|\n",
      "|B0012DR1LU|  712| 3.973314606741573|\n",
      "+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: AMAZON_FASHION\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/AMAZON_FASHION.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "#products with good reviews\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()\n",
    "\n",
    "#products with bad reviews\n",
    "sqlDF3 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|      asin|count|     average_score|\n",
      "+----------+-----+------------------+\n",
      "|B017CHORY8|  573|3.1204188481675392|\n",
      "|B00011QUDE|  902|3.6019955654101996|\n",
      "|B000EG8HLE|  663|3.6440422322775263|\n",
      "|B0010ZBORW|  566| 3.648409893992933|\n",
      "|B000H6A02A|  763|3.6815203145478375|\n",
      "|B00SQ3S6TA|  506|3.7292490118577075|\n",
      "|B000BJ1CGQ|  887|3.8613303269447576|\n",
      "|B0067F28ZW| 1644|3.8899026763990268|\n",
      "|B007EITOSK|  796|3.9020100502512562|\n",
      "|B000WYJTZG| 2091|3.9775227164036346|\n",
      "|B000PARERW|  675| 4.038518518518519|\n",
      "|B00005JS5C| 2154| 4.088672237697307|\n",
      "|B001AMRQ2W|  604| 4.139072847682119|\n",
      "|B00NT0AR7E|  764| 4.151832460732984|\n",
      "|B000FEF1V4|  599| 4.158597662771285|\n",
      "|B000VV1YOY|  860| 4.180232558139535|\n",
      "|B00ATV35SY|  529|4.2041587901701325|\n",
      "|B001AJ6YS2| 1037| 4.215043394406943|\n",
      "|B01DKQAXC0| 4090| 4.217848410757946|\n",
      "|B00120VWTK|  854| 4.251756440281031|\n",
      "+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2: All_Beauty\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/All_Beauty.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "|      asin|count|     average_score|\n",
      "+----------+-----+------------------+\n",
      "|B00Z7TFU90|  609|1.3628899835796386|\n",
      "|B01AFW792E|  922|1.8253796095444685|\n",
      "|B00TF4WVO0|  579| 1.846286701208981|\n",
      "|B00F0D67Z4|  756|1.9947089947089947|\n",
      "|B005NG71K8|  655|2.0595419847328245|\n",
      "|B00AIU4ZDA|  536|2.1138059701492535|\n",
      "|B00KH7JTVM|  820| 2.252439024390244|\n",
      "|B019VZSD4Y|  546|2.2747252747252746|\n",
      "|B00GOKACLC|  785|2.3070063694267517|\n",
      "|B00LL6KW0K|  576|2.3472222222222223|\n",
      "|B01FF0TWCG|  555|2.4108108108108106|\n",
      "|B00CUKEZ5E|  582| 2.429553264604811|\n",
      "|B00FFZD28W| 1052| 2.446768060836502|\n",
      "|B00KJ67ZDK|  618|2.4789644012944985|\n",
      "|B001F9DJOO| 1049|2.4947569113441372|\n",
      "|B00J4YPPW6|  647| 2.497681607418856|\n",
      "|B00ELFDOYO|  510| 2.552941176470588|\n",
      "|B018E06GBO|  515|2.5533980582524274|\n",
      "|B0095VIIKO| 1350|2.5562962962962965|\n",
      "|B009ZBUKHS|  577|2.5563258232235704|\n",
      "+----------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#3: Cell_Phones_and_Accessories\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Cell_Phones_and_Accessories.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Electronics\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Electronics.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Home_and_Kitchen\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Home_and_Kitchen.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: Sports_and_Outdoors\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Sports_and_Outdoors.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7: Video_Games\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Video_Games.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"view1\")\n",
    "\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count, AVG(overall) as average_score FROM view1 WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"view2\")\n",
    "\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT asin, count, average_score FROM view2 WHERE count > 500 ORDER BY average_score ASC\")\n",
    "\n",
    "sqlDF2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
