{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the top review products of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load the metadata at first\n",
    "metaDF = spark.read.json(\"hdfs://orion11:21001/parsed-meta/\")\n",
    "\n",
    "metaDF.createOrReplaceTempView(\"viewMetaData\")\n",
    "\n",
    "metaDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|               title|      asin|         avg_score|\n",
      "+--------------------+----------+------------------+\n",
      "|Dasom Womens Fash...|B0148B7EJ6| 4.907801418439717|\n",
      "|BOX1MM Nickel Fre...|B0006HB4XE| 4.858267716535433|\n",
      "|Oberon Design Tre...|B011PLYVCA| 4.856209150326797|\n",
      "|Working Class Kid...|B01B5BWTNS| 4.844990548204159|\n",
      "|Stephen Joseph Qu...|B00D8FJF6O|4.8274111675126905|\n",
      "|MJ Metals Jewelry...|B00RLSCLJM| 4.826003298515668|\n",
      "|          Big Undies|B0087UIAMK|  4.81294964028777|\n",
      "|MJ Metals Jewelry...|B00G8Q7JZ4| 4.807453416149069|\n",
      "|Vera Bradley Zip ...|B002Z3N1HE| 4.793103448275862|\n",
      "|Back From Bali Wo...|B01BKVWKCS| 4.788732394366197|\n",
      "|Blue Q Women's No...|B01CEZSSXY| 4.788617886178862|\n",
      "|i Play. Boys' Tod...|B00CZ50F3A| 4.780821917808219|\n",
      "|Allett Leather Or...|B00GRDC4BM| 4.771653543307087|\n",
      "| The Hat Jack Woo...|B001XOQTSE| 4.770623742454728|\n",
      "|Leather Original ...|B00GRDC372| 4.764705882352941|\n",
      "|Agimini Classic W...|B00KGMGACO| 4.758620689655173|\n",
      "|Stephen Joseph Go...|B00409FL4M| 4.752475247524752|\n",
      "|Carter's Baby Boy...|B00XCWZBUW| 4.747191011235955|\n",
      "|100% Silk Extra L...|B00GI76P3K| 4.742857142857143|\n",
      "|New Covenant Pray...|B003WJXKTO| 4.739495798319328|\n",
      "+--------------------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1: AMAZON_FASHION\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/AMAZON_FASHION.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: All_Beauty\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/All_Beauty.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Cell_Phones_and_Accessories\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Cell_Phones_and_Accessories.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: Electronics\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Electronics.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5: Home_and_Kitchen\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Home_and_Kitchen.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6: Sports_and_Outdoors\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Sports_and_Outdoors.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7: Video_Games.json\n",
    "\n",
    "#Task: Load the raw data from HDFS, filter out the fields we don't need. Then, write back to the HDFS\n",
    "originalDF = spark.read.json(\"hdfs://orion11:21001/p4/Video_Games.json\")\n",
    "\n",
    "originalDF.createOrReplaceTempView(\"AMAZON_FASHION\")\n",
    "\n",
    "sqlDF1 = spark.sql(\"SELECT asin, COUNT(*) AS count FROM AMAZON_FASHION WHERE verified = True GROUP BY asin\")\n",
    "\n",
    "sqlDF1.createOrReplaceTempView(\"sqlDF1View\")\n",
    "\n",
    "sqlDF2 = spark.sql(\"SELECT * FROM sqlDF1View WHERE count > 100\")\n",
    "\n",
    "sqlDF2.createOrReplaceTempView(\"sqlDF1View2\")\n",
    "\n",
    "sqlDF3 = spark.sql(\"SELECT sqlDF1View2.asin, AVG(AMAZON_FASHION.overall) AS avg_score FROM sqlDF1View2 LEFT JOIN AMAZON_FASHION ON sqlDF1View2.asin = AMAZON_FASHION.asin GROUP BY sqlDF1View2.asin ORDER BY AVG(AMAZON_FASHION.overall) DESC LIMIT 20\")\n",
    "\n",
    "#sqlDF3.show()\n",
    "\n",
    "sqlDF3.createOrReplaceTempView(\"sqlDF3\")\n",
    "\n",
    "showDF1 = spark.sql(\"SELECT title, asin, avg_score FROM sqlDF3 LEFT JOIN viewMetaData ON sqlDF3.asin = viewMetadata.product_id ORDER BY avg_score DESC\")\n",
    "\n",
    "showDF1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
