{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Review and Metadata Parser\n",
    "\n",
    "Before we get started to work on the analysis, we want to parse the original dataset to the format we like. The reviews dataset is from [Amazon review data (2018)](http://deepyeti.ucsd.edu/jianmo/amazon/index.html). The dataset is compressed and about 34 GB. But unfortunately we have to decompresse it because the RDD cannnot work on compressed file. The decompressed size is about 126GB.  \n",
    "\n",
    "The dataset contains a lof of informattion about a review, but in this task we only need the review time, reviewer and product's id. So this process will take these 3 columns and output them to files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "def json_parser(line):\n",
    "    data = json.loads(line)\n",
    "    timestamp = data[\"unixReviewTime\"]\n",
    "    dt = datetime.datetime.fromtimestamp(int(timestamp))\n",
    "    reviewer = data[\"reviewerID\"]\n",
    "    product_id = data[\"asin\"]\n",
    "    review_date =  str(dt.year) + '-' + str(dt.month) \n",
    "    return review_date + '\\t' + reviewer + '\\t' + product_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = sc.textFile(\"hdfs://orion11:12001/amazon-reviews/All_Amazon_Review.json\")\n",
    "text_file.map(lambda line: json_parser(line)).saveAsTextFile(\"hdfs://orion11:12001/amazon-reviews/parsed_data/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "The review's dataset only contains products' id but not products information about what it is. [Amazon review data (2018)](http://deepyeti.ucsd.edu/jianmo/amazon/index.html) also includes the metadata for each products. To make our analysis more comprehensive, the metadata is important.  We will want to parse the metadata in RDD because it is as huge as the review. dataset. The compressed size is 12GB and decompressed size is 110 GB. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata_parser(line):\n",
    "    data = json.loads(line)\n",
    "    asin = ''\n",
    "    brand = ''\n",
    "    title = ''\n",
    "    category = ''\n",
    "    \n",
    "    if 'asin' in data:\n",
    "        asin = data['asin']\n",
    "        \n",
    "    if 'brand' in data:\n",
    "        brand = data['brand']\n",
    "        \n",
    "    if 'title' in data:\n",
    "        title = data['title']\n",
    "    \n",
    "    if 'category' in data:\n",
    "        if len(data['category']) > 0:\n",
    "            category = data['category'][0]\n",
    "    \n",
    "    parsed = {\n",
    "        'product_id': asin,\n",
    "        'category': category, \n",
    "        'brand' : brand,\n",
    "        'title' : title\n",
    "    }\n",
    "    # asin, category, brand, title, description\n",
    "    return json.dumps(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the port\n",
    "metadata = sc.textFile(\"hdfs://orion11:21001/All_Amazon_Meta.json\")\n",
    "metadata.map(lambda line: metadata_parser(line)).saveAsTextFile(\"hdfs://orion11:21001/parsed-meta\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
